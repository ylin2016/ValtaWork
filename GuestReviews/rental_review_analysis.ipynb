{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d03215",
   "metadata": {},
   "source": [
    "# Rental Review Analysis Notebook\n",
    "\n",
    "**Purpose:**\n",
    "This notebook implements an end-to-end pipeline to analyze short-term rental reviews per your request: (1) aggregate multiple reviews per property, (2) summarize them using an LLM (OpenAI example provided), (3) canonicalize/normalize issues, and (4) add embeddings-based clustering to find common issues.  \n",
    "\n",
    "**Notes:** Replace CSV path placeholders and set `OPENAI_API_KEY` if you want to run LLM calls. The notebook is modular so you can swap LLM providers.\n",
    "\n",
    "- find the common issues, such as smoke alarm, sink drain\n",
    "- segment comparison by apartment vs houses, seattle vs remote, 2 bdr vs 3bdr+\n",
    "\n",
    "- check the final solutions for the issues: categorized, and whether it results in client request refund\n",
    "- Check whether 5 stars but leave private note,how about 4 stars with private note\n",
    "- Check whether there are issues raisen but no reviews\n",
    "- trace where some tasks didn't complete in time, and what the reactions from the future guests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daded1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-2.3.0-py3-none-any.whl (999 kB)\n",
      "\u001b[K     |████████████████████████████████| 999 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/ylin/Library/Python/3.9/lib/python/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /Users/ylin/Library/Python/3.9/lib/python/site-packages (2.0.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 79.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 55.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp39-cp39-macosx_11_0_arm64.whl (997 kB)\n",
      "\u001b[K     |████████████████████████████████| 997 kB 65.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "\u001b[K     |████████████████████████████████| 486 kB 73.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting umap-learn\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 20.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 28.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.12.0-py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 36.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jiter<1,>=0.10.0\n",
      "  Downloading jiter-0.11.0-cp39-cp39-macosx_11_0_arm64.whl (305 kB)\n",
      "\u001b[K     |████████████████████████████████| 305 kB 71.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.11 in /Users/ylin/Library/Python/3.9/lib/python/site-packages (from openai) (4.14.1)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 30.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/ylin/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ylin/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ylin/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 63.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.3 MB 75.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "\u001b[K     |████████████████████████████████| 249 kB 31.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.1-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 56.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-11.3.0-cp39-cp39-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 67.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/ylin/Library/Python/3.9/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 8.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.26.0\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2025.9.18-cp39-cp39-macosx_11_0_arm64.whl (286 kB)\n",
      "\u001b[K     |████████████████████████████████| 286 kB 53.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.11.0\n",
      "  Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 73.6 MB 720 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[K     |████████████████████████████████| 564 kB 59.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 50.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.51.2\n",
      "  Downloading numba-0.60.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 49.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /Users/ylin/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Collecting idna>=2.8\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 30.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 21.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 52.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11>=0.16\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading pyyaml-6.0.3-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 67.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 72.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 61.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /Users/ylin/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Downloading llvmlite-0.43.0-cp39-cp39-macosx_11_0_arm64.whl (28.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.8 MB 77.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.41.1\n",
      "  Downloading pydantic_core-2.41.1-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 62.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.2\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp39-cp39-macosx_10_9_universal2.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 36.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 50.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 48.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 42.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 71.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 68.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 65.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "\u001b[K     |████████████████████████████████| 432 kB 63.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading markupsafe-3.0.3-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, tqdm, requests, pyyaml, hf-xet, fsspec, filelock, threadpoolctl, sniffio, scipy, mpmath, MarkupSafe, llvmlite, joblib, huggingface-hub, h11, typing-inspection, tokenizers, sympy, scikit-learn, safetensors, regex, pyparsing, pydantic-core, pillow, numba, networkx, kiwisolver, jinja2, importlib-resources, httpcore, fonttools, cycler, contourpy, anyio, annotated-types, transformers, torch, pynndescent, pydantic, matplotlib, jiter, httpx, distro, umap-learn, tiktoken, sentence-transformers, seaborn, openai\n",
      "Successfully installed MarkupSafe-3.0.3 annotated-types-0.7.0 anyio-4.11.0 certifi-2025.10.5 charset-normalizer-3.4.3 contourpy-1.3.0 cycler-0.12.1 distro-1.9.0 filelock-3.19.1 fonttools-4.60.1 fsspec-2025.9.0 h11-0.16.0 hf-xet-1.1.10 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.35.3 idna-3.11 importlib-resources-6.5.2 jinja2-3.1.6 jiter-0.11.0 joblib-1.5.2 kiwisolver-1.4.7 llvmlite-0.43.0 matplotlib-3.9.4 mpmath-1.3.0 networkx-3.2.1 numba-0.60.0 openai-2.3.0 pillow-11.3.0 pydantic-2.12.0 pydantic-core-2.41.1 pynndescent-0.5.13 pyparsing-3.2.5 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.6.1 scipy-1.13.1 seaborn-0.13.2 sentence-transformers-5.1.1 sniffio-1.3.1 sympy-1.14.0 threadpoolctl-3.6.0 tiktoken-0.12.0 tokenizers-0.22.1 torch-2.8.0 tqdm-4.67.1 transformers-4.57.0 typing-inspection-0.4.2 umap-learn-0.5.9.post2 urllib3-2.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Notebook ready. Install dependencies if needed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 0. Requirements - run this cell once to install packages (uncomment to run)\n",
    "%pip install openai pandas numpy scikit-learn matplotlib seaborn tiktoken sentence-transformers umap-learn\n",
    "\n",
    "print('Notebook ready. Install dependencies if needed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9d48b",
   "metadata": {},
   "source": [
    "## 1) Configuration and imports\n",
    "Update the `CSV_PATH` to point at your reviews CSV. The notebook expects the columns you provided:\n",
    "\n",
    "`Listing, Reservation, Guest.name, checkin_date, checkout_date, booking_platform, createdAt, Overall, Check.in.score, Accuracy, Cleanliness, Communication, Location, Value, Public.Review, PropertyType, Region, BEDROOMS`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "289002a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'OPENAI_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Config - update paths and API key as needed\u001b[39;00m\n\u001b[1;32m     20\u001b[0m review_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ylin/My Drive/Cohost/Data and Reporting/06-Reviews/Data/PropertyReviews.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# update as needed\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# or set here as string (not recommended in notebooks)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m LLM_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-5\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# example; swap if needed\u001b[39;00m\n\u001b[1;32m     23\u001b[0m EMBEDDING_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-embedding-3-small\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# example\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'OPENAI_API_KEY'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# LLM client - optional (OpenAI example included)\n",
    "try:\n",
    "    import openai\n",
    "except Exception:\n",
    "    openai = None\n",
    "\n",
    "# Embedding fallback / clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Config - update paths and API key as needed\n",
    "review_PATH = \"/Users/ylin/My Drive/Cohost/Data and Reporting/06-Reviews/Data/PropertyReviews.xlsx\" # update as needed\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] # or set here as string (not recommended in notebooks)\n",
    "LLM_MODEL = 'gpt-5'  # example; swap if needed\n",
    "EMBEDDING_MODEL = 'text-embedding-3-small'  # example\n",
    "OUTPUT_DIR = Path('notebook_outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f4041",
   "metadata": {},
   "source": [
    "## 2) Load your CSV and preview\n",
    "We'll load the CSV and ensure the expected columns exist. The notebook will create convenient column aliases matching your provided schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1ff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 3926\n",
      "All expected columns present.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Listing</th>\n",
       "      <th>Reservation</th>\n",
       "      <th>Guest.name</th>\n",
       "      <th>checkin_date</th>\n",
       "      <th>checkout_date</th>\n",
       "      <th>booking_platform</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Check.in.score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Communication</th>\n",
       "      <th>Location</th>\n",
       "      <th>Value</th>\n",
       "      <th>Public.Review</th>\n",
       "      <th>PropertyType</th>\n",
       "      <th>Region</th>\n",
       "      <th>BEDROOMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beachwood 1</td>\n",
       "      <td>HMYAAXKZCY</td>\n",
       "      <td>Eƶéquias</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>2023-12-11</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>2023-12-13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great place to stay, I thought I would have pr...</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beachwood 1</td>\n",
       "      <td>HMP9YF2R2A</td>\n",
       "      <td>Aimée</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>2023-08-09</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>2023-08-09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Beautiful space with access to the Puget Sound...</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beachwood 1</td>\n",
       "      <td>HMSWBTETFP</td>\n",
       "      <td>Hong</td>\n",
       "      <td>2023-10-03</td>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>2023-10-05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The unit is perfect, with a great view of the ...</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Listing Reservation Guest.name checkin_date checkout_date  \\\n",
       "0  Beachwood 1  HMYAAXKZCY   Eƶéquias   2023-12-08    2023-12-11   \n",
       "1  Beachwood 1  HMP9YF2R2A      Aimée   2023-08-04    2023-08-09   \n",
       "2  Beachwood 1  HMSWBTETFP       Hong   2023-10-03    2023-10-04   \n",
       "\n",
       "  booking_platform   createdAt  Overall  Check.in.score  Accuracy  \\\n",
       "0           Airbnb  2023-12-13      5.0             5.0       4.0   \n",
       "1           Airbnb  2023-08-09      5.0             5.0       5.0   \n",
       "2           Airbnb  2023-10-05      5.0             5.0       5.0   \n",
       "\n",
       "   Cleanliness  Communication  Location  Value  \\\n",
       "0          5.0            5.0       5.0    5.0   \n",
       "1          5.0            5.0       5.0    5.0   \n",
       "2          5.0            5.0       5.0    5.0   \n",
       "\n",
       "                                       Public.Review PropertyType   Region  \\\n",
       "0  Great place to stay, I thought I would have pr...        Condo  Seattle   \n",
       "1  Beautiful space with access to the Puget Sound...        Condo  Seattle   \n",
       "2  The unit is perfect, with a great view of the ...        Condo  Seattle   \n",
       "\n",
       "   BEDROOMS  \n",
       "0         2  \n",
       "1         2  \n",
       "2         2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load CSV\n",
    "df = pd.read_excel(review_PATH)\n",
    "print('Loaded rows:', len(df))\n",
    "expected_cols = ['Listing','Reservation','Guest.name','checkin_date','checkout_date','booking_platform','createdAt','Overall','Check.in.score','Accuracy','Cleanliness','Communication','Location','Value','Public.Review','PropertyType','Region','BEDROOMS']\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    print('WARNING - missing columns from CSV:', missing)\n",
    "else:\n",
    "    print('All expected columns present.')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f7732",
   "metadata": {},
   "source": [
    "### Quick column mapping\n",
    "Create normalized column names that we'll use in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b54b3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after mapping: ['property_id', 'booking_id', 'guest_name', 'checkin_date', 'checkout_date', 'platform', 'created_at', 'rating', 'checkin_score', 'accuracy_score', 'cleanliness_score', 'communication_score', 'location_score', 'value_score', 'review_text', 'property_type', 'region', 'bedrooms']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_id</th>\n",
       "      <th>booking_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>property_type</th>\n",
       "      <th>region</th>\n",
       "      <th>bedrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beachwood 1</td>\n",
       "      <td>HMYAAXKZCY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great place to stay, I thought I would have pr...</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beachwood 1</td>\n",
       "      <td>HMP9YF2R2A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Beautiful space with access to the Puget Sound...</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beachwood 1</td>\n",
       "      <td>HMSWBTETFP</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The unit is perfect, with a great view of the ...</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   property_id  booking_id  rating  \\\n",
       "0  Beachwood 1  HMYAAXKZCY     5.0   \n",
       "1  Beachwood 1  HMP9YF2R2A     5.0   \n",
       "2  Beachwood 1  HMSWBTETFP     5.0   \n",
       "\n",
       "                                         review_text property_type   region  \\\n",
       "0  Great place to stay, I thought I would have pr...         Condo  Seattle   \n",
       "1  Beautiful space with access to the Puget Sound...         Condo  Seattle   \n",
       "2  The unit is perfect, with a great view of the ...         Condo  Seattle   \n",
       "\n",
       "   bedrooms  \n",
       "0         2  \n",
       "1         2  \n",
       "2         2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Column mapping - adapt if your CSV uses slightly different names\n",
    "col_map = {\n",
    "    'Listing':'property_id',\n",
    "    'Reservation':'booking_id',\n",
    "    'Guest.name':'guest_name',\n",
    "    'checkin_date':'checkin_date',\n",
    "    'checkout_date':'checkout_date',\n",
    "    'booking_platform':'platform',\n",
    "    'createdAt':'created_at',\n",
    "    'Overall':'rating',\n",
    "    'Check.in.score':'checkin_score',\n",
    "    'Accuracy':'accuracy_score',\n",
    "    'Cleanliness':'cleanliness_score',\n",
    "    'Communication':'communication_score',\n",
    "    'Location':'location_score',\n",
    "    'Value':'value_score',\n",
    "    'Public.Review':'review_text',\n",
    "    'PropertyType':'property_type',\n",
    "    'Region':'region',\n",
    "    'BEDROOMS':'bedrooms'\n",
    "}\n",
    "\n",
    "# Apply mapping for columns that exist\n",
    "available_map = {k:v for k,v in col_map.items() if k in df.columns}\n",
    "df = df.rename(columns=available_map)\n",
    "# Ensure key columns exist\n",
    "for c in ['property_id','booking_id','review_text']:\n",
    "    if c not in df.columns:\n",
    "        df[c] = None\n",
    "\n",
    "# Basic cleaning\n",
    "df['review_text'] = df['review_text'].astype(str).fillna('')\n",
    "df['property_id'] = df['property_id'].astype(str)\n",
    "df['booking_id'] = df['booking_id'].astype(str)\n",
    "df['bedrooms'] = pd.to_numeric(df.get('bedrooms', pd.Series(np.nan)), errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "print('Columns after mapping:', df.columns.tolist())\n",
    "df[['property_id','booking_id','rating','review_text','property_type','region','bedrooms']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7dd6f0",
   "metadata": {},
   "source": [
    "## 3) Aggregate multiple reviews per property\n",
    "We'll combine all public reviews per `property_id` into a single text block for summarization. We'll keep individual reviews too for later linking (refunds, tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49869766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped properties: 81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>property_type</th>\n",
       "      <th>region</th>\n",
       "      <th>bedrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beachwood 1</td>\n",
       "      <td>Great place to stay, I thought I would have pr...</td>\n",
       "      <td>4.794872</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beachwood 10</td>\n",
       "      <td>nan\\n\\ngreat communication, beautiful apartmen...</td>\n",
       "      <td>4.272727</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beachwood 3</td>\n",
       "      <td>A really nice, well equipped apartment with a ...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Condo</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    property_id                                        review_text  \\\n",
       "0   Beachwood 1  Great place to stay, I thought I would have pr...   \n",
       "1  Beachwood 10  nan\\n\\ngreat communication, beautiful apartmen...   \n",
       "2   Beachwood 3  A really nice, well equipped apartment with a ...   \n",
       "\n",
       "   avg_rating property_type   region  bedrooms  \n",
       "0    4.794872         Condo  Seattle         2  \n",
       "1    4.272727         Condo  Seattle         1  \n",
       "2    5.000000         Condo  Seattle         1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Aggregate public reviews per property\n",
    "grouped = df.groupby('property_id').agg({\n",
    "    'review_text': lambda texts: '\\n\\n'.join([t for t in texts if str(t).strip()!='']),\n",
    "    'rating': 'mean',\n",
    "    'property_type':'first',\n",
    "    'region':'first',\n",
    "    'bedrooms':'first'\n",
    "}).reset_index().rename(columns={'rating':'avg_rating'})\n",
    "\n",
    "print('Grouped properties:', len(grouped))\n",
    "grouped.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46369f",
   "metadata": {},
   "source": [
    "## 4) LLM summarization function\n",
    "This cell contains a **framework-neutral** summarization wrapper. It uses the OpenAI API if `openai` is installed and an API key is set. Otherwise it will return a mock summary so you can test the pipeline locally.\n",
    "\n",
    "**Output format (JSON):** `{positives:[...], negatives:[...], critical_issues:[...], refund_mentions:[...], sentiment_trend: 'improving'|'stable'|'declining'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab134269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_reviews_with_llm(text, model=LLM_MODEL, openai_client=openai, api_key=OPENAI_API_KEY):\n",
    "    # If no text return empty\n",
    "    if not text or str(text).strip()=='':\n",
    "        return {'positives': [], 'negatives': [], 'critical_issues': [], 'refund_mentions': [], 'sentiment_trend': 'stable'}\n",
    "    # If openai not available or no key, return a mock summary for testing\n",
    "    if openai_client is None or not api_key:\n",
    "        # crude heuristics for demo\n",
    "        positives = []\n",
    "        negatives = []\n",
    "        if 'view' in text.lower() or 'sunset' in text.lower():\n",
    "            positives.append('view')\n",
    "        if 'clean' in text.lower():\n",
    "            positives.append('cleanliness')\n",
    "        if 'parking' in text.lower():\n",
    "            negatives.append('parking')\n",
    "        if 'smoke' in text.lower() or 'marijuana' in text.lower():\n",
    "            negatives.append('smoke odor')\n",
    "            critical = ['smoke odor']\n",
    "        else:\n",
    "            critical = []\n",
    "        return {'positives': positives, 'negatives': negatives, 'critical_issues': critical, 'refund_mentions': [], 'sentiment_trend': 'stable'}\n",
    "    # Real OpenAI call path\n",
    "    openai_client.api_key = api_key\n",
    "    prompt = f\"\"\"You are an assistant analyzing reviews for a short-term rental property.\n",
    "Summarize these reviews. Provide a JSON object with keys:\n",
    "- positives: up to 5 short positive themes\n",
    "- negatives: up to 5 recurring complaints\n",
    "- critical_issues: safety/maintenance issues (short phrases)\n",
    "- refund_mentions: if guests requested refunds or compensation, list brief notes\n",
    "- sentiment_trend: one of improving, stable, declining\n",
    "\n",
    "Reviews:\n",
    "{ text }\n",
    "\n",
    "Return only valid JSON.\"\"\"\n",
    "    try:\n",
    "        resp = openai_client.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{'role':'user','content':prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        raw = resp.choices[0].message['content']\n",
    "        # try parse JSON\n",
    "        parsed = None\n",
    "        try:\n",
    "            parsed = json.loads(raw)\n",
    "        except Exception:\n",
    "            import re\n",
    "            m = re.search(r'(\\{.*\\})', raw, flags=re.S)\n",
    "            if m:\n",
    "                parsed = json.loads(m.group(1))\n",
    "        if parsed is None:\n",
    "            parsed = {'raw': raw}\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        return {'error':'llm_error','detail': str(e), 'raw':''}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216a5dc",
   "metadata": {},
   "source": [
    "### 4b) Run LLM summarization on grouped properties (demo / batch)\n",
    "**Caution:** Running LLM calls may incur cost. If you have many properties, run in batches and set `OPENAI_API_KEY` in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b09e4ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_id</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>llm_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beachwood 1</td>\n",
       "      <td>4.794872</td>\n",
       "      <td>{'error': 'llm_error', 'detail': '\n",
       "\n",
       "You tried ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beachwood 10</td>\n",
       "      <td>4.272727</td>\n",
       "      <td>{'error': 'llm_error', 'detail': '\n",
       "\n",
       "You tried ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beachwood 3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>{'error': 'llm_error', 'detail': '\n",
       "\n",
       "You tried ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    property_id  avg_rating                                        llm_summary\n",
       "0   Beachwood 1    4.794872  {'error': 'llm_error', 'detail': '\n",
       "\n",
       "You tried ...\n",
       "1  Beachwood 10    4.272727  {'error': 'llm_error', 'detail': '\n",
       "\n",
       "You tried ...\n",
       "2   Beachwood 3    5.000000  {'error': 'llm_error', 'detail': '\n",
       "\n",
       "You tried ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Example: run summarization on grouped (here we will run a mock if no API key)\n",
    "summaries = []\n",
    "for _, row in grouped.iterrows():\n",
    "    s = summarize_reviews_with_llm(row['review_text'])\n",
    "    summaries.append(s)\n",
    "\n",
    "grouped['llm_summary'] = summaries\n",
    "grouped[['property_id','avg_rating','llm_summary']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10265c3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mresponses\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a haiku about code.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     reasoning\u001b[38;5;241m=\u001b[39m{ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meffort\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m },\n\u001b[1;32m      8\u001b[0m     text\u001b[38;5;241m=\u001b[39m{ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m },\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39moutput_text)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_client.py:137\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    135\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "result = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a haiku about code.\",\n",
    "    reasoning={ \"effort\": \"low\" },\n",
    "    text={ \"verbosity\": \"low\" },\n",
    ")\n",
    "\n",
    "print(result.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3088c",
   "metadata": {},
   "source": [
    "## 5) Extract critical issues into an issues table and normalize\n",
    "We'll pull `critical_issues` from the LLM summaries and canonicalize them with a simple mapping. Later we cluster similar phrases using embeddings or TF-IDF + KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract issues\n",
    "issue_rows = []\n",
    "for _, r in grouped.iterrows():\n",
    "    summ = r['llm_summary'] or {}\n",
    "    issues = summ.get('critical_issues') if isinstance(summ, dict) else []\n",
    "    if isinstance(issues, str):\n",
    "        issues = [issues]\n",
    "    for iss in issues:\n",
    "        if iss and str(iss).strip():\n",
    "            issue_rows.append({'property_id': r['property_id'], 'issue_raw': str(iss).strip()})\n",
    "issues_df = pd.DataFrame(issue_rows)\n",
    "if issues_df.empty:\n",
    "    print('No critical issues found in summaries (this is OK for demo).')\n",
    "else:\n",
    "    display(issues_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple canonicalization mapping - extend as needed\n",
    "canonical_map = {\n",
    "    'smoke odor':'smoke_alarm_or_odor',\n",
    "    'smoke':'smoke_alarm_or_odor',\n",
    "    'smoke alarm':'smoke_alarm_or_odor',\n",
    "    'sink drain':'sink_drain',\n",
    "    'slow sink':'sink_drain',\n",
    "    'shower small':'bathroom_shower_size',\n",
    "    'door lock':'door_lock',\n",
    "    'keypad not locking':'door_lock',\n",
    "    'parking':'parking',\n",
    "    'noise':'noise'\n",
    "}\n",
    "\n",
    "def canonicalize_issue(s):\n",
    "    s_low = s.lower().strip()\n",
    "    for k,v in canonical_map.items():\n",
    "        if k in s_low:\n",
    "            return v\n",
    "    # fallback: normalized text with spaces -> underscores\n",
    "    return s_low.replace(' ','_')\n",
    "\n",
    "if not issues_df.empty:\n",
    "    issues_df['issue_canonical'] = issues_df['issue_raw'].apply(canonicalize_issue)\n",
    "    issues_df = issues_df.merge(df[['property_id','property_type','region','bedrooms']].drop_duplicates('property_id'), on='property_id', how='left')\n",
    "    issues_df.to_csv(OUTPUT_DIR / 'issues_table.csv', index=False)\n",
    "    issues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd969072",
   "metadata": {},
   "source": [
    "## 6) Embeddings-based clustering (or TF-IDF fallback)\n",
    "If you have an embeddings provider API key (OpenAI or sentence-transformers), use it. Otherwise the notebook falls back to TF-IDF + KMeans. The clusters will help discover common issue themes across all review texts (not only critical_issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare texts for clustering: use all review_texts per property\n",
    "texts = grouped['review_text'].fillna('').tolist()\n",
    "\n",
    "use_openai_embeddings = (openai is not None and OPENAI_API_KEY)\n",
    "\n",
    "if use_openai_embeddings:\n",
    "    # OpenAI embeddings example\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    embs = []\n",
    "    for t in texts:\n",
    "        try:\n",
    "            resp = openai.Embedding.create(input=t, model=EMBEDDING_MODEL)\n",
    "            embs.append(resp['data'][0]['embedding'])\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print('Embedding error', e)\n",
    "            embs.append([0]*1536)\n",
    "    X = np.array(embs)\n",
    "else:\n",
    "    # TF-IDF fallback\n",
    "    vec = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    X = vec.fit_transform(texts).toarray()\n",
    "\n",
    "# Run KMeans for clustering themes\n",
    "n_clusters = min(8, max(2, int(len(texts)/5))) if len(texts)>0 else 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "grouped['cluster'] = kmeans.labels_\n",
    "\n",
    "# Show cluster top terms if TF-IDF used\n",
    "if not use_openai_embeddings:\n",
    "    terms = vec.get_feature_names_out()\n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    clusters_summary = {}\n",
    "    for i in range(n_clusters):\n",
    "        top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "        clusters_summary[f'cluster_{i}'] = top_terms[:8]\n",
    "    print('TF-IDF cluster top terms (sample):')\n",
    "    print(json.dumps(clusters_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8380e3b1",
   "metadata": {},
   "source": [
    "## 7) Segment comparisons & basic analytics\n",
    "Produce breakdowns by `property_type`, `region`, and bedroom buckets (2bdr vs 3bdr+). Also compute refund linkage if you provide refunds data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8477e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bedroom bucket\n",
    "grouped['bedroom_bucket'] = grouped['bedrooms'].apply(lambda x: '2bdr' if x==2 else ('3bdr+' if x>=3 else 'other'))\n",
    "\n",
    "# Cluster counts by segment\n",
    "seg_counts = grouped.groupby(['region','property_type','bedroom_bucket','cluster']).size().reset_index(name='count')\n",
    "seg_counts.head(10).to_csv(OUTPUT_DIR/'cluster_segment_counts.csv', index=False)\n",
    "seg_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dee60e",
   "metadata": {},
   "source": [
    "## 8) Save outputs and next steps\n",
    "This notebook saves outputs to `notebook_outputs/`. Next steps you can run: join with refunds, tasks tables; run LLM summarization in batches; refine canonicalization mapping; build dashboards (Streamlit/Power BI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save grouped with clusters and summaries\n",
    "grouped.to_csv(OUTPUT_DIR/'grouped_properties_with_summaries.csv', index=False)\n",
    "print('Saved grouped data and cluster segment counts to', OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
